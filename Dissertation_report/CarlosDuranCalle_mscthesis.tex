% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the academic year, say 2013/14 say).

\documentclass[ % the name of the author
                    author={Carlos Duran Calle},
                % the name of the supervisor
                supervisor={Dr. Felipe Campelo},
                % the degree programme
                    degree={MSc},
                % the dissertation    title (which cannot be blank)
                     title={Comparative Machine Learning Analysis for Student Dropout Prediction in a Virtual Learning Environment},
                % the dissertation subtitle (which can    be blank)
                  subtitle={Incorporating Student Engagement and Socio-Economic Features},
                % the dissertation     type
                      type={},
                % the year of submission
                      year={2025}]{dissertation}

\begin{document}

% =============================================================================

% This section simply introduces the structural guidelines.  It can clearly
% be deleted (or commented out) if you use the file as a template for your
% own dissertation: everything following it is in the correct order to use 
% as is.

%\section*{Prelude}
%\thispagestyle{empty}






% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).


\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures, tables and algorithms.  The former is a compulsory part of the
% dissertation, but if you do not require the latter they can be suppressed
% by simply commenting out the associated macro.

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms
`%\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Abstract}

{\bf A compulsory section, of at most $1$ page} 
\vspace{1cm} 

\noindent
This section should summarise the project context, aims and objectives,
and main contributions (e.g., deliverables) and achievements.  The goal is to ensure that the 
reader is clear about what the topic is, what you have done within this 
topic, {\em and}\/ {\bf what your view of the outcome is.}

Essentially 
this section is a (very) short version of what is typically covered in more depth in the first 
chapter.  If appropriate, you should include here  
a clear statement of your research hypothesis.  This will obviously differ significantly
for each project, but an example might be as follows:

\begin{quote}
My research hypothesis is that a suitable genetic algorithm will yield
more accurate results (when applied to the standard ACME data set) than 
the algorithm proposed by Jones and Smith, while also executing in less
time.
\end{quote}

\noindent
The latter aspects should (ideally) be presented as a concise, factual 
list of the main points of achievement.  Again the points will differ for each project, but 
an might be as follows:

\begin{quote}
\noindent
\begin{itemize}
\item I spent $120$ hours collecting material on and learning about the 
      Java garbage-collection sub-system. 
\item I wrote a total of $5000$ lines of {\em Python} source code, and associated orchestration scripts. 
\item I designed a new algorithm for computing the non-linear mapping 
      from A-space to B-space using a genetic algorithm.
\item I implemented a version of the algorithm proposed by Jones and 
      Smith (2010), corrected a mistake in it, and 
      compared the results with several alternatives.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

%\chapter*{Summary of Changes}

%{\bf A conditional section, of at most $1$ page} 
%\vspace{1cm} 
%
%If (and only if) the dissertation represents a resubmission (e.g., as the result of
%a resit), then this section is compulsory: the content should summarise all
%non-trivial changes made to the initial submission.  Otherwise you can
%omit it, since {\bf a summary of this type is only needed for resubmissions}.
%
%When included, the section will ideally be used to highlight additional
%work completed, and address criticism raised in any associated feedback.
%Clearly it is difficult to give generic advice about how to do so, but
%an example might be as follows:
%
%\begin{quote}
%\noindent
%\begin{itemize}
%\item Feedback from the initial submission criticised the design and 
%      implementation of my genetic algorithm, stating ``there seems 
%      to have been no attention to computational complexity during the
%      design, and obvious methods of optimisation are missing within
%      the resulting implementation''.  Chapter 3 now includes a
%      comprehensive analysis of the algorithm, in terms of both time
%      and space.  While I have not altered the algorithm itself, I
%      have included a cache mechanism (also detailed in Chapter 3)
%      that provides a significant improvement in average run-time.
%\item I added a feature in my implementation to allow automatic rather
%      than manual selection of various parameters; the experimental
%      results in Chapter 4 have been updated to reflect this.
%\item Questions after the presentation highlighted a range of related
%      work that I had not considered: I have make a number of updates 
%      to Chapter 2, resolving this issue.
%\end{itemize}
%\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Supporting Technologies}

{\bf A compulsory section, of at most $1$ page}
\vspace{1cm} 

\noindent
This section should present a detailed summary, in bullet point form, 
of any third-party resources (e.g., hardware and software components) 
used during the project.  Use of such resources is always perfectly 
acceptable: the goal of this section is simply to be clear about how
and where they are used, so that a clear assessment of your work can
result.  The content can focus on the project topic itself (rather,
for example, than including ``I used \mbox{\LaTeX} to prepare my 
dissertation''); an example is as follows:

\begin{quote}
\noindent
\begin{itemize}

\item I used the {\em Pandas} and {\em Seaborn} public-domian Python Libraries. 

\item I used a parts of the OpenCV computer vision library to capture 
      images from a camera, and for various standard operations (e.g., 
      threshold, edge detection).

\item I used Amazon Web Services for remote storage and processing of data. Specifically, I used:
 \begin{itemize}
 \item Simple Storage Service (S3) for data storage
 \item Elastic Compute Cloud (EC2) for provision of virtual machines
 \item Elastic Beanstalk for scaling and load management
 \item Sagemaker for all the machine learning components of my project. 
 \end{itemize}
 
\item I used \LaTeX\ to format my thesis, via the desktop service {\em TeXstudio}. 
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

%{\bf An optional section, of roughly $1$ or $2$ pages}
\vspace{1cm} 

%\noindent
%Any well written document will introduce notation and acronyms before
%their use, {\em even if} they are standard in some way: this ensures 
%any reader can understand the resulting self-contained content.  
%
%Said introduction can exist within the dissertation itself, wherever 
%that is appropriate.  For an acronym, this is typically achieved at 
%the first point of use via ``Advanced Encryption Standard (AES)'' or 
%similar, noting the capitalisation of relevant letters.  However, it 
%can be useful to include an additional, dedicated list at the start 
%of the dissertation; the advantage of doing so is that you cannot 
%mistakenly use an acronym before defining it.  A limited example is 
%as follows:

\begin{quote}
\noindent
\begin{tabular}{lcl}
SDP                 &:     & Student Dropout Predictor	\\
MOOCs               &:     & Massive Open Online Courses	\\
VLE                 &:     & Virtual Learning Environments	\\
OULAD               &:     & Open University Learning Analytics Dataset	\\
OU                  &:     & Open University	\\
ML                  &:     & Machine Learning	\\
RF                  &:     & Random Forest	\\
LG                  &:     & Logistic Regression	\\
LightGBM            &:     & Light Gradient-Boosting Machine	\\
NN                  &:     & Neural Networks	\\
SE                  &:     & Student Engagement	\\
IMD                 &:     & Index of Multiple Deprivation	\\
TMA                 &:     & Tutor Marked Assessment	\\
KNN                 &:     & K-Nearest Neighbors	\\
KNN                 &:     & K-Nearest Neighbors	\\
KNN                 &:     & K-Nearest Neighbors	\\
KNN                 &:     & K-Nearest Neighbors	\\
%                    &\vdots&                                                                      \\
%${\mathcal H}( x )$ &:     & the Hamming weight of $x$                                            \\
%${\mathbb  F}_q$    &:     & a finite field with $q$ elements                                     \\
$x_i$               &:     & the $i$-th bit of some binary sequence $x$, st. $x_i \in \{ 0, 1 \}$ \\
\end{tabular}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Acknowledgements}

{\bf An optional section, of at most $1$ page}
\vspace{1cm} 

\noindent
It is common practice (although totally optional) to acknowledge any
third-party advice, contribution or influence you have found useful
during your work.  Examples include support from friends or family, 
the input of your Supervisor and/or Advisor, external organisations 
or persons who  have supplied resources of some kind (e.g., funding, 
advice or time), and so on.

\vspace{1cm}
Dave Cliff writes here to say huge thanks to his colleague Dr Dan Page for sharing this \LaTeX\ thesis template, which was originally written by Dan, for Computer Science dissertations. Dave edited Dan's original to better suit the needs of the Data Science MSc: please don't hassle Dan about any of this, but do feel free to contact Dave if you have any questions or comments on it.  

% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter

% -----------------------------------------------------------------------------

\chapter{Introduction}
\label{chap:introduction}

%{\bf A compulsory chapter, roughly 10\% of the total page-count}
\vspace{1cm} 

% putting a \noindent before the first para in each chapter looks nicer.
\noindent
Education is now more widely accessible thanks to online learning.  However, high attrition has continued.  The median completion rate for massive open online courses (MOOCs) is close to $12.6\%$, which has frequently remained low \cite{jordan_massive_2015}.  Large-scale attrition has also been documented in open-university environments.  A study from the Open University (UK) indicates that course-level dropout rates can reach as high as $78\%$ \cite{simpson_can_we_do_better}.  These numbers have inspired timely assistance and predictive systems.

The Open University (OU) released the widely used Open University Learning Analytics Dataset (OULAD), a public dataset for learning analytics research.   OULAD contains information from 22 module presentations and $32,593$ registered students.  Demographics, tests, and daily click-log summaries from the Virtual Learning Environment (VLE) are among these data \cite{kuzilek_OULAD_2017}.   It is currently regarded as the de facto standard for feature engineering and early-warning models in distance learning because of its size and documentation \cite{kuzilek_OULAD_2017}.   For example, Hussain et al. \cite{hussain_student_engagement_prediction_2018} discovered that engagement signals predict outcomes consistently across platforms.   The current study builds on this evidence by predicting engagement-aware early intervention in the OU context using OULAD.

One possible remedy for this issue is machine learning (ML).  Large volumes of student data can be analysed using ML algorithms to find patterns that human observers might miss \cite{holland_ML_1992}.  These algorithms can identify which students are most likely to leave the VLE early by analysing data on their click behaviour, assessment submission, and resource access \cite{baker_educational_2014}.  Dropout prevention becomes predictive rather than reactive as a result of this predictive ability.

\section{Problem Statement}
The prediction task is structured as a three-class classification: Pass, Fail, and Withdrawn.  This framing supports targeted responses (e.g., academic support for likely fail; re-engagement for likely withdrawn) and reflects OULAD's $final\_result$ taxonomy.  This outcome schema and its connection to registration and assessment tables are described in detail in the OULAD documentation \cite{kuzilek_OULAD_2017}. 

There is a noticeable disparity in class.  The Withdrawn class made up $19.1\%$ of the records in the project's processed dataset, making it the minority class for the three-way target. The other classes had $55.5\%$ Pass and $25.4\%$ Fail.  In line with earlier findings that OULAD results are unequally distributed across Pass/Fail/Withdrawn categories, this imbalance makes model training and evaluation more difficult, particularly for recall on the minority class \cite{le_quy_survey_2022}.

Early detection is also necessary.  Delivering interventions before disengagement solidifies has a greater impact.  The importance of early-phase risk detection in reducing withdrawal has been highlighted by previous work in open online courses \cite{martinez-carrascal_using_2023}.  As a result, the objective is to provide actionable, early warning in addition to final outcome prediction.

\section{Research Objectives}
According to earlier studies, student engagement is a significant predictor of both academic success and dropout, and engagement proxies that are derived from VLE activity traces have shown especially good results \cite{hussain_student_engagement_prediction_2018}.  Similarly, when examined alongside engagement patterns, sociodemographic factors like age, region, disability, and prior education have been demonstrated to correlate with withdrawal \cite{kuzilek_OULAD_2017}.

The specific objectives of this project are:
\begin{itemize}
	\item To incorporate the student engagement feature as a meaningful predictor for the ML models.
	\item To determine which socio-demographic factors are significantly associated with student dropout..
	\item To choose a predictive model that can recognise students who are likely to drop out of a course early on.
\end{itemize}

The accomplishment is meaningful beyond the technical achievement. Accurate identification of dropout can also help institutions to determine the best investment of their support resources, develop more effective interventions and enhance the retention of students \cite{kahu_student_engagement_2013}. For students, that support can be the difference between meeting educational goals and falling among the dropout numbers.

\section{ML Approach}
This project uses and compares six widely used ML models with complementary advantages for dropout prediction: Random Forest (RF), an ensemble method known to be robust to high-dimensional data \cite{breiman_rf_2001}; Logistic Regression (LR), to interpret the results for risk factor analysis \cite{harrell_LR_2015}; K-Nearest Neighbors (KNN), to capture local data patterns based on instance-based learning \cite{cover_nearest_1967}; LightGBM, optimized gradient boosting to handle large datasets efficiently \cite{ke_lightgbm_2017}; Support Vector Machine (SVM), capable of dealing with high-dimensional spaces via kernel-based transformations \cite{cortes_svm_1995}; and Neural Networks (NN), to model complex non-linear data relationships \cite{lecun_nn_gradient-applied_1998}.

Every model will be tuned with respect to hyperparameters by GridSearchCV with 5-fold cross-validation. The main evaluation metric is recall of the "Withdrawn" class, as we attempt to identify all at risk students. This emphasis on minority class recall is paramount as missing out a student that requires assistance has higher stakes than sometimes offering help even if it may not be necessary \cite{sokolova_classification_tasks_2009}.

\section{Contributions}
The present study builds upon existing research with three important advances. First, new engagement features are developed to model students' behaviour before their first attempt at the assessment. The $Student Engagement$ (SE) variable is precipitated with academic status along with VLE logs, similar to the approach of Hussain et al. \cite{hussain_student_engagement_prediction_2018}. It is demonstrated in this study that this composite measure allows better capturing student engagement than single traditional measures.

Second, socio-economic variables, in particular the Index of Multiple Deprivation (IMD), are added to explore to what extent dropout risk is influenced by exogenous factors. As compared with the other OULAD studies of Tomasevic et al. \cite{tomasevic_comparison_supervised_data_2020} and Hussain et al. \cite{hussain_student_engagement_prediction_2018} indicated demography, these relations were not further investigated, in contrast socio-economical aspects came out as some of the strongest drop-out predictors.

Third, it was developed a thorough model comparison framework with a unified evaluation schema. All models are trained on the same data split with data-specific class-weighting (Withdrawn: 2.51, Fail: 1.57, Pass: 0.57) to account for class imbalance, therefore fairly comparable and giving insights into a best approach for deployment.

\section{Challenges and Scope}
This research is faced with many challenges. Attention to class imbalance is required so that models do not ignore the minority dropout class. These temporal characteristics require careful feature engineering to generate predictions early enough to allow for interventions. In addition, stakeholders need to be able to understand why students are being flagged as at-risk, which necessitates a balance between model complexity and interpretability.

The complete methodology is presented in this dissertation, covering the process from data preparation through model deployment. Chapter 2 provides technical background on educational data mining and ML algorithms. Chapter 3 outlines the implementation pipeline and feature engineering approach. Chapter 4 reports the results and model comparisons in detail. Chapter 5 offers a critical evaluation of the findings, and Chapter 6 concludes with practical recommendations and directions for future research.

Through systematic comparison and optimization, Logistic Regression is shown to achieve a $66.84\%$ dropout recall, crossing the $60\%$ threshold while keeping interpretability. Such result provides the educational institution with a useful instrument to identify and support at-risk students, thus contributing to the general objective of successful online education.

\paragraph{}
%\noindent
In summary, this project focuses on performing and comparing six ML models to identify the best-performing model for detecting students who have withdrawn from online courses. By emphasizing recall for the 'Withdrawn' class and incorporating engagement and socio-economic factors, the study aims to provide actionable insights that support timely interventions and improve student retention in virtual learning environments. 
%The chapter should conclude with a concise bullet point list that 
%summarises the aims, objectives, {\bf and achievements}\/ of your work. 
% -----------------------------------------------------------------------------

\chapter{Technical Background}
\label{chap:background}

%{\bf A compulsory chapter, roughly 20\% of the total page-count}
\vspace{1cm} 

\noindent
This chapter offers a theory based on student dropout prediction in VLE. Key concepts that comprise educational data mining, multi-class classification problems, and ML algorithms are discussed. The discussion is the technical basis for a comparison among dropout prediction models.

\section{Educational Data Mining in VLE}
Educational Data Mining in VLE explores the wealth of behavioural data produced by students (ie, clickstream patterns and resource access statistics) for the purpose of gaining knowledge on how they learn \cite{romero_data_2013}. Browsing and resource navigation behaviour have been shown to be strong predictors of academic performance, such as course completion \cite{gasevic_learning_2016}. Combining this behavioural data with a learners' demographics and assessment results substantially enhances prediction accuracy of dropout models \cite{viberg_current_2018}.

The temporal nature of this data is important. Early warning signs of at-risk students can be well-predicted from early-stage behaviour patterns \cite{conijn_predicting_2017}. The high-frequency of VLE data streams on the other hand requires advanced preprocessing to deal with missing values, irregular sampling intervals, and varying engagement patterns across different student populations \cite{sclater_learning_2016}. In addition, integrating VLE interaction data with socio-economic features improves model stability and classification performance, and paves the way for a more systematic feature engineering in educational prediction systems \cite{hlosta_modellingVLE_2018}.

\section{The OULAD Dataset Architecture}
OULAD is released as a set of CSV tables that can be joined through surrogate keys, enabling a student‑centric relational view across demographics, registrations, assessments, learning materials, and VLE interactions \cite{kuzilek_OULAD_2017}, the database schema can be appreciated in the Figure~\ref{database_scheme} \cite{nafea_enhancing_2023}. The public release contains $22$ module‑presentations, $32,593$ students, and $10,655,280$ daily click summaries, supporting at‑scale analyses of behaviour and outcomes. All dates are stored as offsets relative to the module‑presentation start, which simplifies time‑window filters such as “before the first assessment”.

\subsection{Table Descriptions}
Summaries below follow the official data descriptor for OULAD \cite{kuzilek_OULAD_2017}.
\begin{itemize}
	\item Demographics and final result per student‑module presentation (e.g., gender, age band, prior education, credits, disability, and $final\_result$).
	\item Registration and unregistration days for each student in each presentation; empty unregistration implies completion.
	\item Submission day and score (0–100) for each assessment attempted.
	\item Per‑assessment metadata: type (TMA/CMA/Exam), cut‑off day, and weight; non‑exam assessments sum to $100$ and exams are treated separately.
	\item Daily counts of interactions ($sum\_click$) by student with specific learning materials ($id\_site$).
	\item The catalog of VLE materials with activity type and planned availability windows ($week\_from$, $week\_to$).
	\item Module and presentation identifiers with presentation length in days.
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../Visualizations/Report/2_1_database_scheme.png}
	\caption[Database schema of OULAD dataset]{Database schema of OULAD dataset (reproduced from \cite{nafea_enhancing_2023}).}
	\label{database_scheme}
\end{figure}

\section{Student Engagement Indicators}
Simple, interpretable flags were engineered to capture early excellence and activity in ways that align with common early‑warning practices in learning analytics \cite{macfadyen_mining_2010}.
Mathematical notations are added no understand following equations.

\begin{itemize}
	\item Let $\mathbf{1}[\cdot]$ be the indicator function (1 if the condition is true, else 0).
	\item Let the first graded assessment in a presentation be indexed by $a_1$
	\item Let score $a_1$ be the student’s first‑assessment score (0–100).
	\item Let $date(a_1)$ be its cut‑off day.
	\item Let $C_{pre}$ be the student’s total $sum\_click$ with $studentVle.date < date(a_1)$.
	\item Let $\mu_{pre}$ be the mean of $C_{pre}$ within the same ($code\_module$, $code\_presentation$).
	\item These columns and date conventions are defined in OULAD.
\end{itemize}

\subsection{Excellent Score Indicator}
A binary indicator for early academic excellence was defined as:
\begin{equation}
	\text{excellent\_Score} = \mathbf{1}\!\left[ \, \text{score}(a_1) \geq 70 \, \right]
	\tag{2.1}
\end{equation}
According to the Scholaro database, The Open University awards a “Merit” scale when is higher than $70\%$ \cite{open_university_grading}. This threshold marks, where is categorised as distinction mark for OU, clearly high performance while keeping the rule easy to interpret; early assessment performance has been shown to be informative for final outcomes and targeted support.

\subsection{Active in VLE Indicator}
A binary indicator for above‑average pre‑assessment activity was defined as:
\begin{equation}
	\text{active\_in\_VLE} = \mathbf{1}\!\left[ \, C_{\text{pre}} > \mu_{\text{pre}} \, \right]
	\tag{2.2}
\end{equation}
Clicks prior to the first assessment are counted and compared with the cohort’s mean for the same module‑presentation; VLE click behaviours have repeatedly shown predictive value for course performance, including in OULAD‑based studies \cite{liu_predicting_2023}.

\subsection{Student Engagement Indicator}
A composite engagement flag was defined with a logical OR:
\begin{equation}
	\text{student\_engagement} = \mathbf{1}\!\left[ \, \text{excellent\_Score} = 1 \, \lor \, \text{active\_in\_VLE} = 1 \, \right]
	\tag{2.3}
\end{equation}
This rule fires if either early excellence or above‑average activity is observed, a common, conservative design for early‑warning heuristics that favours recall of potentially successful or engaged students \cite{macfadyen_mining_2010}.

\paragraph{Why these signals}
The pair (grade-based, behaviour-based) captures complementary facets of engagement and aligns with evidence that clickstream engagement and formative performance jointly inform later achievement \cite{baker_benefits_2020}.

\section{Stratification and Encoding}
\subsection{Cohort‑Outcome Stratification}
A stratified hold‑out split was performed per cohort so each module–presentation kept its original class mix, and the target values (withdrawn/fail/pass) was included in the strata to preserve outcome prevalence within every cohort. Class‑imbalance concerns motivated stratification to reduce variance and avoid misleading metrics on under‑represented outcomes \cite{he_learning_2009}. The split was executed before any encoding to prevent information leakage from validation/test back into training \cite{kaufman_leakage_2012}. The implementation relied on scikit‑learn splitters with a fixed random state for reproducibility \cite{pedregosa_scikit-learn_2011}. In the project, three alternatives were examined and Strategy 2 (cohort + outcome) was retained due to its better distribution in comparison with the Strategy 1 (cohort); a triple‑key variant Strategy 3 what include “courses per term” was discarded due to tiny cells that break stratified sampling.

\subsection{Feature Encoding}
Encoding was intentionally minimal and applied only to a selected set of categorical variables that it was found relevant for the SDP: $region$, $highest\_education$, $imd\_band$, $age\_band$, and $disability$; numerical and binary features were left unchanged to keep the signal simple. Nominal fields were one‑hot encoded to avoid imposing order, while ordered bands ($imd\_band$, $age\_band$) were ordinal‑encoded to preserve ranks in a compact form \cite{scikit_encoding}. Encoders were fit only on the training split and then applied to test dataset, with unseen categories mapped safely to an “Unknown” bucket to avoid runtime errors and leakage \cite{scikit_encoding}. The steps were orchestrated with $scikit‑learn$ library and the encoded matrices for the train and test dataset, and labels were persisted for downstream modelling.


\section{Multi-Class Classification}
In education, multi-class classification is complicated since the standard binary techniques are not effective in modelling the interactions among various performance levels, which leads to specialized algorithms \cite{fernandez_learning_2018}. Model collection is in turn influencing the ordinal nature of effects (e.g., from “Pass to Fail”) might need attentive attention at some stage in model training \cite{liu_exploratory_2009}. Standard decomposition methods may bias such type of ordered data, which validate why it is useful to apply native multi-class algorithms in order to predict student performances accurately \cite{krawczyk_learning_2016}.

Class imbalance is a critical challenge in educational data, as traditional accuracy metrics can be misleading by failing to reflect poor performance on key minority classes, such as "Withdrawn" students \cite{haixiang_learning_2017}. Proper evaluation of these models requires specialized metrics, such as class-specific recall and macro-averaged F1 scores, to ensure a balanced assessment \cite{luque_impact_2019}. Advanced techniques, including strategic oversampling combined with ensemble methods, have been shown to significantly improve the identification of these at-risk students \cite{galar_review_2012}.

\section{Multi-Class Evaluation Metrics}
The class-specific performance metrics can offer valuable insights on how a model performs and work well for imbalanced data. Among these, precision and recall are core indicators for assessment of classification effectiveness. For a given class $i$, precision is defined as the ratio of true positive predictions to all positive predictions, as shown in Equation 2.1. This concept is introduced in \cite{sokolova_classification_tasks_2009}.
\begin{equation}
	\text{Precision}_{i} = \frac{TP_{i}}{TP_{i} + FP_{i}}
	\tag{2.1}
\end{equation}
\begin{center}
	where $TP_{i}$ represents true positives and $FP_{i}$ represents false positives for class $i$.
\end{center}

On the other hand, recall is defined as the ratio of the actual positive instances that are classified as positive which is given in Equation 2.2 \cite{sokolova_classification_tasks_2009}. In educational applications, recall for the "Withdrawn" class is especially important as it measures the model's capacity to detect students who need early support, while having high recall rates ensures that such at-risk students are not missed out even with elevated false positive rates \cite{grandini_metrics_2020}.
\begin{equation}
	\text{Recall}_{i} = \frac{TP_{i}}{TP_{i} + FN_{i}}
	\tag{2.2}
\end{equation}
\begin{center}
	where $TP_{i}$ represents true positives and $FN_{i}$ denotes false negatives for class $i$.
\end{center}

Weighted metrics compensate for the class imbalance by incorporating the relative frequency of each class contributing to the overall performance measure and deliver a model effectiveness evaluation that is more representative of how well it generalizes to all categories \cite{jeni_facing_2013}. Weighted recall is calculated according to the Equation 2.3 \cite{tantisripreecha_novel_2022}.
\begin{equation}
	\text{Weighted Recall} = \sum_{i} w_{i} \times \text{Recall}_{i}
	\tag{2.3}
\end{equation}
\begin{center}
	where $w_{i}$ is the weight for class $i$, usually based on its frequency in the dataset.
\end{center}

Weighted F1-score is the harmonic mean of precision and recall for each class $i$ \cite{tantisripreecha_novel_2022} \cite{opitz_closer_2024} as in the Equation 2.4. These balanced metrics allow us to measure the performance level for all categories of evaluation indicators while still being sensitive to the practical important of minority class detection in the educational intervention systems \cite{krawczyk_learning_2016}.
\begin{equation}
	\text{Weighted F1} = \sum_{i} w_{i} \times F1_{i}
	\tag{2.4}
\end{equation}
\begin{center}
	where $F1_{i} = 2 \times \frac{\text{Precision}_{i} \times \text{Recall}_{i}}{\text{Precision}_{i} + \text{Recall}_{i}}$ 
	and $w_{i}$ represents the weight assigned to class $i$.
\end{center}

\section{Strategies for Imbalanced Data}
Balanced class weight is one of the basic approaches to address the imbalance of the dataset where the importance of each class is adjusted automatically and inversely proportional to the frequency of that class and the mathematical foundation is given as it is written in the Equation 2.5 \cite{akor_hierarchical_2025}.
\begin{equation}
	w_{i} = \frac{n_{\text{samples}}}{n_{\text{classes}} \times n_{\text{samples}_{i}}}
	\tag{2.5}
\end{equation}
\begin{center}
	where $w_{i}$ denotes the weight for class $i$, $n_{\text{samples}}$ represents the total number of samples, 
	$n_{\text{classes}}$ indicates the number of classes, and $n_{\text{samples}_{i}}$ represents the number of samples in class $i$.
\end{center}

Customized scoring metrics deepen this approach by considering domain-specific preferences, by dropout-specific optimization forcing the maximization of the recall for withdrawal detection via biasing the learning criterion in favour of at-risk students and focusing less on false negatives \cite{orooji_predicting_2019}.

Early warning signs do exist to prevent high-risk youngsters and to intervene early. In the context of dropout prediction, the imbalance class is the minority dropout class, which results in heavy emphasize on recall to detect the dropout students for timely intervention \cite{lee_machine_2019}. Unbalanced-learning techniques (e.g., resampling or class weighting) generally improve recall for the rare class (dropout) at the expense of precision \cite{orooji_predicting_2019}, which is the same precision-recall trade-off this work aims to find the best level of.

In the area of dropout early-detection, balancing the class distribution among minority and majority instances should be considered desirably by using class weights, which disadvantage the majority and advantage the minority class, e.g., the weight of the minority class being higher than $1$ and the weight of the majority class being equal to $1$ is common \cite{hlosta_ouroboros_2017}. More recent losses, e.g., the Class-Balanced Loss, have tried to provide a more principled re-weighting beyond naïve inverses only when classes are long-tailed \cite{cui_class-balanced_2019}. For the current study it is beneficial as we would like to investigate alternative to ad-hoc multipliers, used for balanced weighting, like x$1.2$ for the minority class “Withdrawn” and x$0.8$ for the majority class “Pass”. The multipliers are domain specific choices of the experimentations: there is one setting that uses no multipliers (balanced weighting), and one that applies the multipliers. Lastly, note that overall accuracy may be deceptive of imbalanced data, as Precision and Recall can better assess minority-class detection quality \cite{saito_precision-recall_2015}.

\section{ML Algorithms for SDP}
Ensemble learning exploits multiple models to reduce variance and improve generalization; in RF, bootstrap aggregation (bagging) draws resamples of the training set and averages randomized decision trees, providing robustness and out‑of‑bag error estimation \cite{breiman_bagging_1996} \cite{breiman_rf_2001}. Feature importance in RF is commonly derived from impurity decreases or permutation‑based accuracy drops, enabling screening of influential predictors in heterogeneous educational data \cite{breiman_rf_2001}. Gradient‑boosting frameworks fit weak learners sequentially under an additive model to minimize a differentiable loss, while LightGBM accelerates this process using histogram‑based splits, leaf‑wise growth, and gradient‑based one‑side sampling for scalability on high‑dimensional, large cohorts \cite{friedman_gbm_2001} \cite{ke_lightgbm_2017}. LR models the log‑odds of class membership and estimates coefficients by maximum likelihood, yielding interpretable effects as odds ratios and supporting regularization for stability when features are correlated \cite{cox_regression_1958} \cite{nelder_generalized_1972}. SVM maximize the geometric margin via convex quadratic programming and use kernel mappings to induce flexible non‑linear decision boundaries while controlling capacity through the margin and kernel parameters \cite{cortes_svm_1995}. 

Instance‑based learning with KNN classifies by a majority (plurality) vote among the K closest samples, so decision boundaries are local and sensitive to the choice of distance metric and K, with common metrics including Euclidean, Manhattan, and Mahalanobis distances \cite{cover_nearest_1967} \cite{aha_instance-based_1991}. Multi‑layer perceptrons (a type of NN) compose affine transformations with non‑linear activations and are trained end‑to‑end by backpropagation to minimize a supervised loss, with universal approximation guaranteeing expressive capacity under sufficient width or depth \cite{rumelhart_learning_1986} \cite{hornik_multilayer_1989}. These foundations motivate comparing RF, LightGBM, LR, SVM, KNN, and NN for dropout prediction, balancing interpretability, non‑linear modelling power, and computational efficiency under class imbalance typical of at‑risk cohorts \cite{krawczyk_learning_2016}.

\section{Model Evaluation and Comparison Methodologies}
Performance was estimated with stratified 5‑fold cross‑validation \cite{kohavi_study_1995} on the training split to preserve class proportions and reduce variance in imbalanced data, with a fixed stratified train–test split reused for all models to ensure comparability \cite{luque_impact_2019}. Hyperparameter search was executed within cross‑validation (GridSearchCV) and final metrics were computed once on the untouched test split to limit selection bias \cite{varma_bias_2006}. All models were evaluated on identical folds and scoring rules to enable fair, paired comparisons across the pipeline \cite{demsar_statistical_2006}. Hyperparameters were optimized via exhaustive GridSearchCV with five folds, parallel execution, dropout‑recall as the objective, and class‑weighting to emphasize minority cases \cite{kohavi_study_1995} \cite{krawczyk_learning_2016}.

Regularisation and early stopping are treated as crucial mechanisms to curb over‑fitting and stabilise learning dynamics \cite{prechelt_early_2012}. In LR, the strength of regularisation is controlled by the inverse‑penalty parameter $C$, with $L1/L2$ penalties shaping sparsity and shrinkage behaviour \cite{ng_feature_2004}. For SVM, the soft‑margin constant $C$ and kernel scale gamma are tuned to balance margin violations and function smoothness \cite{cortes_svm_1995}. In decision trees and ensemble methods, maximum depth and the number of estimators are adjusted to limit variance and improve robustness \cite{breiman_rf_2001}. In gradient boosting, the learning rate and number of boosting rounds jointly govern additive model capacity and error reduction \cite{friedman_gbm_2001}. For KNN, the neighbourhood size $k$ and the choice of distance metric determine locality and boundary smoothness \cite{cover_nearest_1967}. In multilayer perceptrons, hidden‑layer size and weight decay are set under backpropagation to control capacity and enhance generalisation \cite{krogh_simple_1991}. 

% -----------------------------------------------------------------------------

\chapter{Methodology and Implementation}
\label{chap:execution}

\vspace{1cm} 
\noindent
This chapter describes the methodical process used to create and assess ML models for student dropout prediction, covering everything from data preprocessing to model deployment and validation.

\section{Overview and Research Design}
With an emphasis on early intervention capabilities, this study applies a ML pipeline intended to forecast the likelihood of student dropout in online learning environments. By creating predictive models that can identify at-risk students before they drop out of their courses, the study tackles the crucial problem of student retention in higher education.

\subsection{Research Methodology Framework}
The approach uses supervised learning and compares six different machine ML in a methodical manner: RF, Multinomial LR, KNN, LightGBM Gradient Boosting, SVM, and NN.  Robust evaluation across various learning paradigms, ranging from distance-based and deep learning approaches to ensemble methods and linear models, is ensured by this thorough algorithmic comparison.

From data intake to model winning selection, the research framework uses a structured pipeline (Figure~\ref{project_pipeline}), integrating modular Python architecture for scalability and reproducibility.  Each algorithm undergoes rigorous hyperparameter optimisation using GridSearchCV with 5-fold cross-validation to guarantee fair comparison and optimal performance extraction.

\subsection{Dropout Optimization}
To enable successful early intervention systems, the main goal is to maximise dropout recall $\geq60\%$. Since false negatives, or missing at-risk students, have more serious repercussions than false positives, or failing students who might succeed, in educational intervention contexts, this metric gives priority to identifying students who will drop out. In order to balance practical resource constraints with effective early intervention capabilities, the $60\%$ threshold was set especially for this project. This ensures that roughly two-thirds of at-risk students are identified while maintaining manageable caseloads for institutional support services.

\subsection{Class Imbalance Challenge}
With $19.1\%$ dropout students (minority class), $25.4\%$ failing students, and $55.5\%$ passing students (majority class), the dataset represents a serious class imbalance issue. Specific optimisation techniques, such as unique class weighting schemes and dropout-focused evaluation metrics, are needed to address this imbalance. By using class weights of roughly $2.51$x for dropouts, $1.57$x for failures, and $0.57$x for passes across various algorithms, the research employs customised strategies to make sure minority class detection is not overpowered by majority class dominance.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\textwidth]{../Visualizations/Report/3_1_project_pipeline.png}
	\caption{The Seven-Phase Project Pipeline}
	\label{project_pipeline}
\end{figure}

\section{Data Architecture and Modular Design}
As shown in Figure~\ref{project_pipeline}, the implementation uses a methodical seven-phase pipeline architecture intended for comprehensive SDP. From the intake of raw data to the deployment of models, this modular design guarantees repeatable processes while preserving data integrity across the ML pipeline.

\subsection{Phase 1: Data Ingestion and Validation}
Seven CSV files containing learning analytics data are automatically loaded at the start of the pipeline. Validation protocols check column structures and data integrity.

\subsection{Phase 2: Data Preprocessing}
Predictive modelling is based on core data transformations. By determining the earliest TMA (Tutor Marked Assessment) dates for each course-presentation combination, assessment filtering establishes standardised prediction windows for early intervention. Consistency across diverse data sources is ensured by data type conversions and missing value handling. Only participants who are actively participating during assessment periods are kept by student filtering protocols, which eliminate inactive participants.

\subsection{Phase 3: Feature Engineering}
From raw learning analytics, sophisticated feature creation algorithms produce predictive indicators. Before assessment deadlines, click-stream data is compiled by VLE engagement metrics to identify patterns in behaviour. Academic performance indicators establish engagement thresholds and binary flags for excellence (scores $\geq70$). The $courses\_per\_term$ feature provides context for student capacity analysis by quantifying the distribution of academic load.

\subsection{Phase 4: Stratification and Encoding}
Advanced stratification techniques preserve demographic representation while maintaining class proportions across cohort boundaries. With $19.1\%$ dropout representation, the chosen cohort+outcome strategy guarantees balanced train-test splits. The specific transformations used in categorical encoding are binary encoding for dichotomous variables, ordinal encoding for hierarchical relationships, and one-hot encoding for nominal variables.

\subsection{Phase 5: Model Optimization}
Using thorough hyperparameter optimisation, a systematic algorithm comparison assesses six different ML techniques. 5-fold cross-validation in GridSearchCV guarantees equitable comparison across various learning paradigms. Custom scoring metrics use specific weighting strategies to manage class imbalance while giving priority to dropout recall optimisation.

\subsection{Phase 6: Comparative Evaluation}
A thorough model evaluation analyses performance across six algorithms using a variety of visualisation techniques. Direct performance comparison is made possible by horizontal bar charts that compare key metrics (dropout recall, dropout precision, at-risk recall, weighted F1). Multi-metric performance profiles for each model are displayed using five-dimensional radar plots, which also highlight the models' advantages and disadvantages. Runtime efficiency scatter plots determine the best algorithms for various deployment scenarios by analysing trade-offs between computational cost and performance. With comprehensive confusion matrices for every algorithm, three-panel performance dashboards categorise models into three performance tiers: Excellent $\geq60\%$, Good $40-60\%$, and Needs Work $<40\%$.

\subsection{Phase 7: Model Selection}
The comparative analysis in Phase 6 is the source of the systematic evaluation criteria used in evidence-based model selection. Dropout recall performance ($\geq60\%$ threshold), computational efficiency, and deployment feasibility are given top priority during the selection process. With a $66.84\%$ dropout recall, superior efficiency (4-minute training time), and interpretability advantages, LR is the best option. Runtime efficiency analysis, practical deployment considerations, and performance tier classification (Excellent, Good, Needs Work) are among the selection criteria. While underperforming algorithms (NN at $18.67\%$ recall) are not taken into consideration for production, runner-up models (SVM with $63.09\%$ recall) offer backup options. The final choice strikes a balance between operational needs for actual educational intervention systems and predictive performance.

\section{Data Preprocessing and Feature Engineering}
A systematic six-stage pipeline is implemented by the data preprocessing and feature engineering phases (Phase $2$ and Phase $3$ in Figure~\ref{project_pipeline}), which convert raw learning analytics into predictive features for early intervention systems (Figure~\ref{features_pipeline}). Prior to determining academic outcomes, this creates behavioural indicators and temporal prediction windows that capture patterns of student engagement.

\subsection{Stage 1: Integrating Data Sources}
The pipeline starts with the thorough integration of five main data sources: student registrations, VLE interaction logs, assessment structure and student assessment metadata, and demographic data. The original dataset includes over $10$ million VLE interactions and $32,593$ student registrations across $22$ courses.

\subsection{Stage 2: Defining the Initial Assessment Period}
For every course-presentation combination, this stage identifies the first TMA dates. In order to determine the earliest assessment dates for $22$ courses, this processes $106$ TMA assessments. By eliminating participants who withdrawn prior to their initial assessment opportunity, active student filtering guarantees that attention is directed towards students who might profit from intervention. Consequently, $27,725$ students in total match these filters.

\subsection{Stage 3: Extracting Behavioural Features}
Up until the initial evaluation, this stage shows how three fundamental behavioural dimensions (VLE interaction, academic performance, and course load), were extracted. In order to determine the total and average clicks per student in relation to a course-presentation baseline, VLE interaction analysis analyses $2,587,468$ clicks that took place prior to the first assessment deadlines. Academic performance extraction uses a merit threshold analysis (threshold $70$) and records the initial assessment results. Students taking multiple concurrent courses are identified through course load quantification, which also provides context for their academic capacity. For this data, the maximum number of concurrent courses is $2$; it was not found students with three or more courses at the same presentation time.

\subsection{Stage 4: Creating Indicator}
The fourth step filters the $27,725$ students and converts continuous behavioural measures into interpretable binary indicators. Merit-level performance ($\geq70$ points) is attained by $16,687$ students ($60.2\%$ of the total) according to the academic excellence classification ($excellent\_Score$). $9,870$ students ($35.6\%$) whose platform engagement surpasses course-specific averages are flagged by the VLE activity classification ($active\_in\_{VLE}$). Additionally, a course load analysis ($courses\_per\_term$) was added to quantify the academic workload of $1,270$ students ($4.6\%$) who are managing multiple concurrent courses.

\subsection{Stage 5: Aggregating an Engagement Indicator}
In stage five, various pathways to academic success are captured by combining individual indicators into composite measures. By using logical OR operations to combine academic excellence and VLE activity, the unified engagement feature ($student\_engagement$) finds $19,932$ students ($71.9\%$) who are either performing well or showing strong platform engagement. This composite approach acknowledges that various engagement patterns may lead to students' success.

\subsection{Stage 6: Engineering the Final Training Dataset}
The last step of data preparation creates a validated, ML-ready dataset of $27,725 students$ and $24$ features by combining preprocessed data with engineered behavioural and demographic features. The distribution of classes in this dataset is $55.5\%$ Pass, $25.4\%$ Fail, and $19.1\%$ Withdrawn. With the withdrawn students representing a crucial minority class, it draws attention to the problem of class imbalance. Now that the dataset has been cleaned and optimised, it is ready for the next round of ML pipelines, where specific techniques will be used to predict outcomes for this important minority group.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../Visualizations/Report/3_2_features_pipeline.png}
	\caption{The Six-Stage Engineering Feature Pipeline}
	\label{features_pipeline}
\end{figure}

\section{Data Stratification and Encoding Strategy}

\subsection{Stratification Analysis and Strategy Selection}
Three stratification techniques are evaluated methodically to maximise the preservation of class distribution. Using course-presentation combinations, Strategy 1 uses basic cohort-based stratification. Using a combination of cohort and outcome stratification, Strategy 2 maintains the distributions of academic outcomes and demographics. Triple stratification using cohort$+$outcome$+$courseload variables is attempted in Strategy 3.

\paragraph{Strategy Selection:} Comparative analysis shows that Strategy 2 (Cohort + Outcome) is the best course of action. Since many cohort-outcome-courseload combinations contain only one student, triple stratification (Strategy 3) fails because combined groupings have insufficient sample sizes. As shown in Table~\ref{tab:class_distribution}, Strategy 2 effectively preserves the crucial $19.1\%$ dropout representation across train-test splits while maintaining class proportions.

\paragraph{Stratified Sampling Implementation:}  The chosen strategy guarantees equitable representation for both academic results and course cohorts. The train-test split allocation maintains proportional class representation by using a $80\%-20\%$ distribution. This approach ensures sufficient samples for minority class optimisation in later model training stages while maintaining demographic diversity.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		\textbf{Strategy} & \makecell{\textbf{Withdrawn} \\ \textbf{(Class 0)}} & \makecell{\textbf{Fail} \\ \textbf{(Class 1)}} & \makecell{\textbf{Pass} \\ \textbf{(Class 2)}} & \textbf{Status} \\
		\hline
		Original Dataset & 19.1\% & 25.4\% & 55.5\% & Baseline \\
		\hline
		Strategy 1: Cohort Only & 19.3\% & 25.0\% & 55.7\% & Acceptable \\
		\hline
		Strategy 2: Cohort + Outcome & 19.1\% & 25.4\% & 55.5\% & \textbf{Selected} \\
		\hline
		Strategy 3: Cohort + Outcome + Courseload & - & - & - & Failed \\
		\hline
	\end{tabular}
	\caption{Class Distribution Comparison Across Stratification Strategies}
	\label{tab:class_distribution}
\end{table}


\subsection{Categorical Encoding Methodology}
Using the $encoding\_utils$ module, tailored encoding techniques handle various categorical variable types. Region variables are treated as nominal categories with no intrinsic ordering through one-hot encoding. Ordinal encoding is used to preserve hierarchical relationships across socioeconomic and demographic dimensions while maintaining ranked sequences for the variables of education, age, and IMD band. For dichotomous classification, disability status uses binary encoding. Maintaining feature alignment and preventing data leakage are achieved by encoding consistency across train-test splits. Table~\ref{tab:encoding_strategy} provides an overview of this.

\paragraph{Dataset Export} Systematic model comparison is made possible by encoded datasets exporting as standardised CSV files ($X\_train\_encoded.csv$, $X\_test\_encoded.csv$, $y\_train.csv$, $y\_test.csv$). The encoding procedure preserves the temporal validity set in earlier preprocessing stages while generating consistent feature matrices appropriate for a range of algorithm requirements.

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Variable Type} & \textbf{Variables} & \textbf{Encoding Method} \\
		\hline
		Nominal & Region & One-Hot Encoding \\
		\hline
		Ordinal & Education, Age, IMD Band & Ordinal Encoding \\
		\hline
		Binary & Disability & Binary Encoding \\
		\hline
	\end{tabular}
	\caption{Encoding Strategy Matrix}
	\label{tab:encoding_strategy}
\end{table}

\section{Class Weighting Strategy}
Class imbalance in educational datasets, where withdrawn students make up only $19.1\%$ of the dataset population, requires strategic weighting methods that are implemented through a two-stage calculation process that combines custom multipliers with sklearn's balanced weighting. First, balanced weights are calculated using sklearn's $compute\_class\_weight('balanced')$ function to produce baseline values proportional to inverse class frequencies. Next, strategic multiplier application is applied to improve dropout detection capabilities.  This is demonstrated in Algorithm~\ref{alg:custom_weights}, where the multiplier values ($1.2$x, $1.0$x, $0.8$x) were chosen to reduce majority class dominance by 20\% and increase dropout class sensitivity by 20\%.  With this setup, the class balance is changed from the initial balanced weights $\{0: 1.74, 1: 1.31, 2: 0.60\}$ to the optimised custom weights $\{0: 2.09, 1: 1.31, 2: 0.48\}$.

\begin{algorithm}[h]
	\KwIn{$y\_train$, $classes$}
	\KwOut{Custom class weights}
	
	$balanced\_weights \leftarrow compute\_class\_weight(`balanced', classes, y\_train)$\;
	
	$custom\_weights \leftarrow \{ $\;
	\Indp
	$0 : balanced\_weights[0] \times 1.2$ \tcp{Dropout class boost}	
	$1 : balanced\_weights[1] \times 1.0$ \tcp{Fail class unchanged}	
	$2 : balanced\_weights[2] \times 0.8$ \tcp{Pass class reduction}
	\Indm
	$\}$\;
	
	Resulting weights: $\{0: 2.09, 1: 1.31, 2: 0.48\}$\;
	
	\caption{Computation of custom class weights for ML training.}
	\label{alg:custom_weights}
\end{algorithm}


\subsection{Class Weight implementation}
Three different categories based on native support capabilities are revealed by class weight implementation across six chosen algorithms, as shown in Table~\ref{tab:class_weight}.  Through built-in $class\_weight$ parameters that automatically modify loss functions and sample importance during training, RF, LG, LightGBM, and SVM offer direct implementation.  Because of its distance-based architecture, KNN has limited implementation capabilities and does not support native class weights. Custom weights can be declared, but they are not successfully integrated during prediction.  With no alternative strategies like weighted sampling or custom loss functions implemented in the examined notebooks, NN ($MLPClassifier$) is still unsupported and expressly lacks class weight functionality.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c|l|l|}
		\hline
		\textbf{Algorithm} & \textbf{Native Support} & \textbf{Implementation Method} & \textbf{Usage Status} \\
		\hline
		Random Forest           & $\checkmark$ & \texttt{class\_weight} parameter & \textbf{Active} \\
		\hline
		Logistic Regression     & $\checkmark$ & \texttt{class\_weight} parameter & \textbf{Active} \\
		\hline
		LightGBM                & $\checkmark$ & \texttt{class\_weight} parameter & \textbf{Active} \\
		\hline
		Support Vector Machine  & $\checkmark$ & \texttt{class\_weight} parameter & \textbf{Active} \\
		\hline
		K-Nearest Neighbors     & $\times$     & Use another type of weighting          & \textbf{Not applied} \\
		\hline
		Neural Networks         & $\times$     & Not supported by MLPClassifier   & \textbf{Not applied} \\
		\hline
	\end{tabular}
	\caption{Class Weight Support by Algorithm}
	\label{tab:class_weight}
\end{table}

\section{ML Model Selection and Optimization}
Six ML algorithms were compared methodically and rigorously optimised for student dropout prediction during the model selection phase.

\subsection{Algorithm Selection and Optimization}
The following six algorithms are complementary in their ability to handle the complexities of educational data mining: RF for robustness to feature interactions, Multinomial LR for interpretable probabilistic outputs, KNN for local behavioural patterns, LightGBM for advanced gradient boosting, SVM for non-linear boundary detection, and NN for deep learning capabilities. In order to address the crucial importance of accurately identifying at-risk students, each algorithm underwent systematic hyperparameter optimisation using GridSearchCV with 5-fold cross-validation, prioritising dropout recall performance ($\geq60\%$) while maintaining balanced accuracy through custom class weights: dropout class weighted $2.5$x, fail class $1.5$x, and pass class $0.6$x. Each ML model's hyperparameter search space is displayed in Table~\ref{tab:algo_search}. The hyperparameter search encompassed 2,032 total parameter combinations using parallel processing with 4 CPU cores.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|p{0.4\textwidth}|l|}
		\hline
		\textbf{Algorithm} & \textbf{Key Parameters} & \textbf{Search Range} & \textbf{Search Space Size} \\
		\hline
		& n\_estimators        & 100--500                       &  \\
		\textbf{RF} & max\_depth           & 10--25                         & 480 combinations \\
		& min\_samples\_split  & 2--10                          &  \\
		& class\_weight        & balanced / custom              &  \\
		\hline
		& C (regularization)   & 0.01--100                      &  \\
		\textbf{LR} & penalty            & L1 / L2 / ElasticNet          & 450 combinations \\
		& solver               & SAGA / liblinear               &  \\
		& max\_iter            & 1000--5000                     &  \\
		\hline
		& n\_neighbors         & 3--21                          &  \\
		\textbf{KNN} & metric             & euclidean / manhattan / minkowski & 90 combinations \\
		& weights              & uniform / distance             &  \\
		& algorithm            & ball\_tree / kd\_tree / brute  &  \\
		\hline
		& n\_estimators        & 100--500                       &  \\
		\textbf{LightGBM} & learning\_rate      & 0.01--0.2                      & 512 combinations \\
		& max\_depth           & 3--8                           &  \\
		& subsample            & 0.7--1.0                       &  \\
		\hline
		& C                    & 0.01--100                      &  \\
		\textbf{SVM} & kernel            & linear / RBF / polynomial     & 180 combinations \\
		& gamma                & scale / auto / numeric         &  \\
		& degree               & 2--4 (poly only)               &  \\
		\hline
		& hidden\_layer\_sizes & (50)--(150) + multilayer       &  \\
		\textbf{NN} & alpha (L2 penalty) & 0.0001--1.0                    & 320 combinations \\
		& learning\_rate\_init & 0.0001--0.1                    &  \\
		& solver               & adam / lbfgs                   &  \\
		\hline
	\end{tabular}
	\caption{Hyperparameter search spaces and search sizes for each algorithm.}
	\label{tab:algo_search}
\end{table}

%\begin{figure}[h]
%\centering
%foo
%\caption{This is an example figure.}
%\label{fig}
%\end{figure}
%
%\begin{table}[h]
%\centering
%\begin{tabular}{|cc|c|}
%\hline
%foo      & bar      & baz      \\
%\hline
%$0     $ & $0     $ & $0     $ \\
%$1     $ & $1     $ & $1     $ \\
%$\vdots$ & $\vdots$ & $\vdots$ \\
%$9     $ & $9     $ & $9     $ \\
%\hline
%\end{tabular}
%\caption{This is an example table.}
%\label{tab}
%\end{table}
%
%\begin{algorithm}[h]
%\For{$i=0$ {\bf upto} $n$}{
%  $t_i \leftarrow 0$\;
%}
%\caption{This is an example algorithm.}
%\label{alg}
%\end{algorithm}
%
%\begin{lstlisting}[float={t},caption={This is an example listing.},label={lst},language=C]
%for( i = 0; i < n; i++ ) {
%  t[ i ] = 0;
%}
%\end{lstlisting}
%
%This is an example sub-section;
%the following content is auto-generated dummy text.
%Notice the examples in Figure~\ref{fig}, Table~\ref{tab}, Algorithm~\ref{alg}
%and Listing~\ref{lst}.

% -----------------------------------------------------------------------------

\chapter{Critical Evaluation}
\label{chap:evaluation}

{\bf A topic-specific chapter, roughly 30\% of the total page-count} 
\vspace{1cm} 

\noindent
This chapter is intended to evaluate what you did.  The content is highly 
topic-specific, but for many projects will have flavours of the following:

\begin{enumerate}
\item functional  testing, including analysis and explanation of failure 
      cases,
\item behavioural testing, often including analysis of any results that 
      draw some form of conclusion wrt. the aims and objectives,
      and
\item evaluation of options and decisions within the project, and/or a
      comparison with alternatives.
\end{enumerate}

\noindent
This chapter often acts to differentiate project quality: even if the work
completed is of a high technical quality, critical yet objective evaluation 
and comparison of the outcomes is crucial.  In essence, the reader wants to
learn something, so the worst examples amount to simple statements of fact 
(e.g., ``graph X shows the result is Y''); the best examples are analytical 
and exploratory (e.g., ``graph X shows the result is Y, which means Z; this 
contradicts [1], which may be because I use a different assumption'').  As 
such, both positive {\em and}\/ negative outcomes are valid {\em if} presented 
in a suitable manner.

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

{\bf A compulsory chapter,  roughly 10\% of the total page-count}
\vspace{1cm} 

\noindent
The concluding chapter(s) of a dissertation are often underutilized because they're 
too often left too close to the deadline: it is important to allocate enough time and 
attention to closing off the story, the narrative, of your thesis.

Again, there is no single correct way of closing a thesis. 

One good way of doing this is to have a single chapter consisting of three parts:

\begin{enumerate}
\item (Re)summarise the main contributions and achievements, in essence
      summing up the content.
\item Clearly state the current project status (e.g., ``X is working, Y 
      is not'') and evaluate what has been achieved with respect to the 
      initial aims and objectives (e.g., ``I completed aim X outlined 
      previously, the evidence for this is within Chapter Y'').  There 
      is no problem including aims which were not completed, but it is 
      important to evaluate and/or justify why this is the case.
\item Outline any open problems or future plans.  Rather than treat this
      only as an exercise in what you {\em could} have done given more 
      time, try to focus on any unexplored options or interesting outcomes
      (e.g., ``my experiment for X gave counter-intuitive results, this 
      could be because Y and would form an interesting area for further 
      study'' or ``users found feature Z of my software difficult to use,
      which is obvious in hindsight but not during at design stage; to 
      resolve this, I could clearly apply the technique of Bloggs {\em et al.}.
\end{enumerate}

Alternatively, you might want to divide this content into two chapters: a penultimate chapter with a title such as ``Further Work" and then a final chapter ``Conclusions". Again, there is no hard and fast rule, we trust you to make the right decision. 

And this, the final paragraph of this thesis template, is just a bunch of citations, added to show how to generate a BibTeX bibliography. Sources that have been randomly chosen to be cited here include:
%\cite{miller_etal_2018_clojure,webber_marwan_2015,touretzky_2013_lisp,eckmann_etal_1987,marwan_2011,vach_2015,shiller_2017,vytelingum_2006,tesfatsion_2002,rust_etal_1992}.




% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a database) then imported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.

\backmatter

%\bibliographystyle{unsrt}
\bibliography{sample_bibtex.bib}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendices; these are 
% the same as chapters in a sense, but once signalled as being appendices via
% the associated macro, LaTeX manages them appropriately.

\appendix

\chapter{An Example Appendix}
\label{appx:example}

Content which is not central to, but may enhance the dissertation can be 
included in one or more appendices; examples include, but are not limited
to

\begin{itemize}
\item lengthy mathematical proofs, numerical or graphical results which 
      are summarised in the main body,
\item sample or example calculations, 
      and
\item results of user studies or questionnaires.
\end{itemize}

\noindent
Note that in line with most research conferences, the examiners are not
obliged to read such appendices.

% =============================================================================

\end{document}
