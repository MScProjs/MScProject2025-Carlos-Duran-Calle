# MScProject - Student Assessment Analysis

A comprehensive machine learning analysis system for student performance prediction using demographic data, VLE engagement metrics, and assessment scores.

## ğŸ¯ Project Overview

This project analyzes student performance data to predict academic outcomes using machine learning models. It examines relationships between demographic characteristics, Virtual Learning Environment (VLE) engagement, assessment scores, and final results through multiple classification models.

### Key Research Questions
- How do demographic factors influence student success?
- What is the relationship between VLE engagement and academic performance?
- Which variables are the strongest predictors of student outcomes?
- How do different machine learning models compare in predicting student results?

## ğŸ“ Project Structure

```
MScProject/
â”œâ”€â”€ README.md                           # Project documentation
â”œâ”€â”€ Data/                               # ğŸ“Š Dataset and processed outputs
â”‚   â”œâ”€â”€ assessments.csv                 # Assessment metadata
â”‚   â”œâ”€â”€ courses.csv                     # Course information
â”‚   â”œâ”€â”€ studentAssessment.csv           # Student assessment scores
â”‚   â”œâ”€â”€ studentInfo.csv                 # Student demographics
â”‚   â”œâ”€â”€ studentRegistration.csv         # Course registration data
â”‚   â”œâ”€â”€ studentVle.csv                  # VLE interaction logs
â”‚   â”œâ”€â”€ vle.csv                         # VLE resource information
â”‚   â”œâ”€â”€ output/                         # ğŸ“ Processed training/test data
â”‚   â”‚   â”œâ”€â”€ processed_data.csv          # Cleaned dataset
â”‚   â”‚   â”œâ”€â”€ X_train_encoded.csv         # Training features
â”‚   â”‚   â”œâ”€â”€ X_test_encoded.csv          # Test features
â”‚   â”‚   â”œâ”€â”€ y_train.csv                 # Training labels
â”‚   â”‚   â””â”€â”€ y_test.csv                  # Test labels
â”‚   â””â”€â”€ model_metrics/                  # ğŸ¤– Model outputs and evaluation
â”‚       â”œâ”€â”€ models/                     # ğŸ“ Trained model files (.pkl)
â”‚       â”‚   â”œâ”€â”€ random_forest_optimized.pkl
â”‚       â”‚   â”œâ”€â”€ multinomial_logistic_regression_optimized.pkl
â”‚       â”‚   â”œâ”€â”€ knn_optimized.pkl
â”‚       â”‚   â”œâ”€â”€ lightgbm_optimized.pkl
â”‚       â”‚   â”œâ”€â”€ svm_optimized.pkl
â”‚       â”‚   â”œâ”€â”€ neural_network_optimized.pkl
â”‚       â”‚   â””â”€â”€ *_scaler.pkl            # Feature scalers for applicable models
â”‚       â”œâ”€â”€ metrics/                    # ğŸ“ Performance metrics (JSON/CSV)
â”‚       â”‚   â”œâ”€â”€ *_metrics.json          # Model performance scores
â”‚       â”‚   â”œâ”€â”€ *_confusion_matrix.json # Confusion matrices
â”‚       â”‚   â”œâ”€â”€ *_coefficients.json     # Model coefficients/feature importance
â”‚       â”‚   â””â”€â”€ *_feature_importance.csv
â”‚       â”œâ”€â”€ reports/                    # ğŸ“ Classification reports (JSON)
â”‚       â””â”€â”€ *_USAGE_INSTRUCTIONS.txt    # Model usage guides
â”‚
â”œâ”€â”€ Documentation/                      # ğŸ“‹ Project documentation
â”‚   â””â”€â”€ Project_Plan-Carlos_Duran.pdf   # Project planning document
â”‚
â”œâ”€â”€ Dissertation_report/                # ğŸ“„ Dissertation and research documentation
â”‚   â””â”€â”€ [Dissertation files]            # Research papers, reports, and analysis documents
â”‚
â”œâ”€â”€ Python_files/                      # ğŸ Core analysis modules
â”‚   â”œâ”€â”€ __init__.py                     # Package initialization
â”‚   â”œâ”€â”€ config_file.py                  # Configuration settings
â”‚   â”œâ”€â”€ data_loader.py                  # Data loading utilities
â”‚   â”œâ”€â”€ data_processor.py               # Data cleaning and preprocessing
â”‚   â”œâ”€â”€ encoding_utils.py               # Feature encoding for ML models
â”‚   â”œâ”€â”€ analysis_engine.py              # Statistical analysis functions
â”‚   â””â”€â”€ visualization.py                # Plotting and visualization tools
â”‚
â”œâ”€â”€ Notebooks/                          # ğŸ““ Analysis workflow notebooks
â”‚   â”œâ”€â”€ 01_data_ingest_cleaning.ipynb           # Data ingestion and cleaning
â”‚   â”œâ”€â”€ 02_visual_analysis.ipynb                # Exploratory data analysis
â”‚   â”œâ”€â”€ 03_data_stratification_encoding.ipynb   # Data preparation for ML
â”‚   â”œâ”€â”€ 04_model_random_forest.ipynb            # Random Forest model
â”‚   â”œâ”€â”€ 05_model_multi_logistic_regression.ipynb # Logistic Regression model
â”‚   â”œâ”€â”€ 06_model_knn.ipynb                      # K-Nearest Neighbors model
â”‚   â”œâ”€â”€ 07_model_lightGBM.ipynb                 # LightGBM model
â”‚   â”œâ”€â”€ 08_model_SVM.ipynb                      # Support Vector Machine model
â”‚   â”œâ”€â”€ 09.model_neural_networks.ipynb          # Neural Network model
â”‚   â””â”€â”€ X1_model_comparison_analysis.ipynb      # Comprehensive model comparison
â”‚
â””â”€â”€ Visualizations/                     # ğŸ“ˆ Generated plots and charts
    â”œâ”€â”€ Report/                         # ğŸ“ Dissertation report visualizations
    â”‚   â”œâ”€â”€ 2_1_database_scheme.png            # Database schema diagram
    â”‚   â”œâ”€â”€ 3_1_project_pipeline.png           # Overall project pipeline
    â”‚   â”œâ”€â”€ 3_2_features_pipeline.png          # Feature engineering pipeline
    â”‚   â”œâ”€â”€ 4_1_data_class_distribution.png    # Target class distribution analysis
    â”‚   â”œâ”€â”€ 4_2_weighted_data_class.png        # Class weighting strategy
    â”‚   â”œâ”€â”€ 4_3_model_performance_comparison.png # Model performance comparison
    â”‚   â”œâ”€â”€ 4_4_model_training_time.png        # Model training time analysis
    â”‚   â”œâ”€â”€ 4_5_model_runtime_gridSearchCV.png # Hyperparameter tuning runtime
    â”‚   â”œâ”€â”€ 4_6_radar_plot_comparison.png      # Radar plot model comparison
    â”‚   â”œâ”€â”€ 4_7_feature_importance.png         # Feature importance analysis
    â”‚   â”œâ”€â”€ A1_Gender_plot.png                 # Gender distribution analysis
    â”‚   â”œâ”€â”€ A1_Age_band_plot.png               # Age distribution analysis
    â”‚   â”œâ”€â”€ A1_HE_plot.png                     # Higher education background
    â”‚   â”œâ”€â”€ A1_Disability_plot.png             # Disability status analysis
    â”‚   â”œâ”€â”€ A1_IMD_band_plot.png               # IMD band distribution
    â”‚   â””â”€â”€ A1_Region_plot.png                 # Regional distribution analysis
    â”‚
    â””â”€â”€ Model_comparison/               # ğŸ“ Model comparison visualizations
        â”œâ”€â”€ 01_class_distribution.png          # Target class distribution
        â”œâ”€â”€ 02_model_comparison.png            # Model performance comparison
        â”œâ”€â”€ 03_class_level_performance.png     # Per-class performance metrics
        â””â”€â”€ 04_confusion_matrices.png          # Confusion matrices grid
```

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install pandas numpy matplotlib seaborn plotly scikit-learn jupyter lightgbm
```

### Running the Analysis
1. **Data Setup**: Place CSV files in the `Data/` folder
2. **Complete Pipeline**: Open `Notebooks/01_data_ingest_cleaning.ipynb` and run sequentially through `X1_model_comparison_analysis.ipynb`
3. **Individual Models**: Run specific model notebooks (04-09) for focused analysis
4. **Custom Analysis**: Import modules from `Python_files/` for programmatic use

## ğŸ“Š Models & Results

### Implemented Models
- **Random Forest** - Ensemble tree-based classifier
- **Logistic Regression** - Linear classification with regularization
- **K-Nearest Neighbors** - Instance-based learning
- **LightGBM** - Gradient boosting framework
- **Support Vector Machine** - Kernel-based classification
- **Neural Networks** - Multi-layer perceptron

### Key Features
- **Feature Engineering**: VLE engagement metrics, demographic encoding
- **Model Optimization**: Hyperparameter tuning with cross-validation
- **Performance Evaluation**: Accuracy, precision, recall, F1-score, confusion matrices
- **Feature Importance**: Analysis of predictive factors across models
- **Comprehensive Comparison**: Head-to-head model performance analysis

## ğŸ“ˆ Outputs

### Model Artifacts
- Trained models saved as `.pkl` files
- Feature scalers for models requiring normalization
- Performance metrics in JSON format
- Classification reports with detailed per-class statistics

### Visualizations
- Model performance comparison charts
- Confusion matrices for all models
- Feature importance rankings
- Class distribution analysis

### Data Products
- Processed and encoded datasets
- Train/test splits ready for ML workflows
- Feature engineering pipeline outputs

## ğŸ”§ Customization

The modular design allows for easy modification:
- **Add new models**: Create new notebook following the established pattern
- **Modify features**: Update `encoding_utils.py` and `data_processor.py`
- **Change evaluation metrics**: Modify `analysis_engine.py`
- **Custom visualizations**: Extend `visualization.py`

## ğŸ“‹ Workflow

1. **Data Ingestion** â†’ Clean and validate raw CSV files
2. **Exploration** â†’ Visual analysis and feature understanding
3. **Preprocessing** â†’ Stratification, encoding, and train/test splits
4. **Modeling** â†’ Train and optimize individual models
5. **Evaluation** â†’ Compare performance across all models
6. **Deployment** â†’ Saved models ready for production use

---

**ğŸ¯ Goal**: Predict student academic outcomes (Withdrawn/Fail/Pass/Distinction) using demographic and engagement data through optimized machine learning models.

## ğŸ“‹ Conclusion

This project addresses the critical challenge of high attrition rates in virtual learning environments through a comprehensive machine learning pipeline designed to identify at-risk students early and enable timely interventions. Using the Open University Learning Analytics Dataset, the analysis classifies student outcomes into three categories: Withdrawn (class 0), Fail (class 1), and Pass (class 2).

### Key Approach
- **Engagement Indicators**: Created three interpretable engagement metrics: excellent score (â‰¥70), VLE activity, and composite student engagement flags
- **Contextual Factors**: Incorporated socioeconomic and demographic variables (highest education, IMD band, disability, age, region)
- **Balanced Methodology**: Employed cohort and outcome stratified data splitting to maintain class proportions
- **Comprehensive Model Evaluation**: Assessed six machine learning algorithms with custom class weights and dropout-focused metrics

### Results
The Multinomial Logistic Regression model demonstrated exceptional performance for early-warning applications with:
- **66.8% recall** for identifying Withdrawn students
- **Efficient training time** (~4 minutes)
- **Interpretable coefficients** for actionable insights
- **Optimized hyperparameters** through stratified 5-fold GridSearchCV

### Impact
This research contributes to educational data mining by providing a robust, interpretable solution for early student risk identification, supporting institutional efforts to improve retention and student success through data-driven interventions.