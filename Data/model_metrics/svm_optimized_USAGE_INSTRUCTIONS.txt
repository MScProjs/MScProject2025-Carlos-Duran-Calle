
SUPPORT VECTOR MACHINE (SVM) MODEL USAGE INSTRUCTIONS
====================================================

Model Information:
- Model Name: svm_optimized
- Model Type: Support Vector Machine (Optimized)
- Model File: ../Data\model_metrics\models\svm_optimized.pkl
- Scaler File: ../Data\model_metrics\models\svm_optimized_scaler.pkl

‚ö†Ô∏è  CRITICAL: SVM REQUIRES FEATURE SCALING!

LOADING THE MODEL:
-----------------
```python
import joblib
import pandas as pd
import numpy as np

# Load BOTH the trained model AND the scaler
model = joblib.load('../Data\model_metrics\models\svm_optimized.pkl')
scaler = joblib.load('../Data\model_metrics\models\svm_optimized_scaler.pkl')

# MANDATORY: Scale new data using the SAME scaler
X_new_scaled = scaler.transform(X_new)

# Make predictions on scaled data
predictions = model.predict(X_new_scaled)
prediction_probabilities = model.predict_proba(X_new_scaled)
```

‚ö†Ô∏è  NEVER skip feature scaling! SVM will give incorrect results on unscaled data.

BEST HYPERPARAMETERS:
-------------------
{
  "C": 0.1,
  "class_weight": {
    "0": 2.0939343875383525,
    "1": 1.312270737190865,
    "2": 0.48051561188290415
  },
  "gamma": "scale",
  "kernel": "rbf"
}

PERFORMANCE METRICS:
------------------
- Test Accuracy: 0.4821
- Test F1-Weighted: 0.4881
- Test F1-Macro: 0.4126
- Best CV Score (dropout_recall): 0.6309

SVM-SPECIFIC INSIGHTS:
---------------------
- Kernel type: rbf
- Regularization parameter (C): 0.1
- Gamma parameter: scale
- Total support vectors: 20,569
- Support vector percentage: 92.7%
- Class balancing: Not applied

SUPPORT VECTORS BY CLASS:
------------------------
Class 0: 4,008 support vectors
Class 1: 5,601 support vectors
Class 2: 10,960 support vectors


FEATURE IMPORTANCE:
------------------
Feature importance analysis not available for rbf kernel.
Linear SVM provides coefficient-based importance.
For non-linear kernels, consider permutation importance.


CLASS DISTRIBUTION:
------------------
Class 0: 4,237 samples (19.1%)
Class 1: 5,634 samples (25.4%)
Class 2: 12,309 samples (55.5%)


FILES SAVED:
-----------
- Model: ../Data\model_metrics\models\svm_optimized.pkl
- Scaler: ../Data\model_metrics\models\svm_optimized_scaler.pkl
- Metrics: ../Data\model_metrics\metrics\svm_optimized_metrics.json
- Classification Report: ../Data\model_metrics\reports\svm_optimized_classification_report.json
- Confusion Matrix: ../Data\model_metrics\metrics\svm_optimized_confusion_matrix.json
- Coefficients: ../Data\model_metrics\metrics\svm_optimized_coefficients.json
- Usage Instructions: This file

PREPROCESSING PIPELINE:
---------------------
1. Load your scaler: scaler = joblib.load('../Data\model_metrics\models\svm_optimized_scaler.pkl')
2. Scale features: X_scaled = scaler.transform(X_raw)
3. Make predictions: predictions = model.predict(X_scaled)

NOTES:
-----
- This model was optimized using GridSearchCV
- Total combinations tested: 24
- Cross-validation: 5-fold
- Optimization runtime: 80.4 minutes
- Primary scoring metric: dropout_recall
- Class imbalance ratio: 2.91:1

‚ö†Ô∏è  CRITICAL SVM REQUIREMENTS:
   - Feature scaling is MANDATORY
   - Both model AND scaler files are required
   - Apply scaler to ALL new data before prediction

üéØ SVM ADVANTAGES:
   - Excellent for high-dimensional data
   - Memory efficient (uses support vectors only)
   - Versatile kernels for different data patterns
   - Strong theoretical foundation

‚ö†Ô∏è  SVM CONSIDERATIONS:
   - Slower training than tree-based models
   - Requires careful hyperparameter tuning
   - Sensitive to feature scaling
   - Can be memory intensive for large datasets
